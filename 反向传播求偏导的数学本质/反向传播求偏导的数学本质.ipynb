{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播求偏导的数学本质\n",
    "\n",
    "本文拟解决一直困扰许多人的反向传播过程中对矩阵的求导，在这篇论文中，我将详细解释每一步操作背后的数学原理，在学会这篇文章中的内容之后，你可以轻松地写出任何layer的反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、纠正错误\n",
    "在神经网络中，只有实数对矩阵的偏导，而不存在矩阵对矩阵的偏导。对于反向传播的所有的求导，我们都要转化成实数对矩阵的偏导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、一些基本的矩阵运算和求导法则\n",
    "\n",
    "### 1.矩阵乘法\n",
    "\n",
    "矩阵乘法有很多不同的表示方式，此处为了便于本文的解释说明，采取列的线性组合这一角度来进行介绍。\n",
    "#### 1. 矩阵右乘列向量——对矩阵的每一列做线性组合\n",
    "\n",
    "假设A是一个 $n \\times m$的矩阵，x是一个列向量，我们将A表示成列的形式，那么：\n",
    "\n",
    "$$\n",
    "y = Ax = \\begin{bmatrix}\n",
    "            | & | &  &| \\\\\n",
    "            a_1 & a_2 & ... & a_m \\\\\n",
    "            | & | &  & |  \\\\\n",
    "            \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_m \\\\ \\end{bmatrix} = \\begin{bmatrix} \\\\ a_1 \\\\ \\\\ \\end{bmatrix}x_1 + \\begin{bmatrix} \\\\ a_2 \\\\ \\\\ \\end{bmatrix}x_2 + ... + \\begin{bmatrix} \\\\  a_m \\\\ \\\\ \\end{bmatrix}x_m\n",
    "$$\n",
    "\n",
    "注意此处y是A中的每一列的线性组合，其相应系数为x中的对应元素， y的大小为n*1\n",
    "\n",
    "\n",
    "#### 2. 矩阵相乘\n",
    "\n",
    "我们可以将右矩阵的每一列与左矩阵相乘， A为$n \\times k$ ，B为$k \\times m$, 那么\n",
    "\n",
    "$$\n",
    "C = AB = A\\begin{bmatrix}\n",
    "            | & | &  &| \\\\\n",
    "            b_1 & b_2 & ... & b_m \\\\\n",
    "            | & | &  & |  \\\\\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix}\n",
    "            | & | &  &| \\\\\n",
    "            Ab_1 & Ab_2 & ... & Ab_m \\\\\n",
    "            | & | &  & |  \\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.实数对矩阵的偏导\n",
    "\n",
    "假设$y \\in R$,  $X \\in R^{m \\times n}$,那么 y对X的求导$\\frac{d y}{dX}$:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    \\frac{y}{X_{11}} & \\frac{y}{X_{12}} & \\frac{y}{X_{13}} \\\\\n",
    "    \\frac{y}{X_{21}} & \\frac{y}{X_{22}} & \\frac{y}{X_{23}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "我们可以看到，实数对矩阵求导的结果和矩阵的大小一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、 神经网络review\n",
    "\n",
    "神经网络中的基本单元是一个神经元，每个神经元接收上一层多个神经元的信号，不过这些信号都有一个权重。我们可以简单地认为这个权重大于0的时候前一个神经元的输出被传播到当前神经元中，而当权重小于0的时候前一个神经元的输出就被截断了。\n",
    "\n",
    "<img src = \"neuron_inputs.png\">\n",
    "\n",
    "而我们在神经网络中接触较多的却是layer这一概念，将多个相同类型的神经元组合成一个神经元。 但实际上，某个layer上的每个神经元都有其自身的一个权重，而且互相没有任何关系，表示成layer只是为了方便整个网络的结构表示。\n",
    "\n",
    "\n",
    "所以在我们学习神经网络的时候，应该去关注每个神经元的操作，这样才能对各个数学操作背后的本质有所了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了说明反向传播中求导的问题，我们先以全连接层来说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"fully_connected.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入层有D个神经元，全连接层有C个神经元。我们可以将输入表示成1*D的向量，而全连接层的每一个神经元都有D个权重，我们将其表示成D*C的矩阵，其第i列表示第i个神经元的权重。\n",
    "\n",
    "那么我们可以得到：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_1 x_2 ... x_C \\end{bmatrix} \\begin{bmatrix} W_{11} & W_{21} & ... & W_{C1} \\\\  W_{12} & W_{22} & ... & W_{C2}  \\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\  W_{1D} & W_{2D} &  ... & W_{CD} \\end{bmatrix} = \\begin{bmatrix} y_1 & y_2 & \\ddots & y_c \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中$W_{ij}$表示第i个神经元的第j个权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
